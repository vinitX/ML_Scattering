{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Mixture Density Network\n",
    "\n",
    "An extension of Bishops' classic MDN prediction task to 2-dimensions.\n",
    "\n",
    "The idea in this task is to predict a the value of a two inverse sine functions simultaneously. This function has multiple real-valued solutions at each point, so the ANN model needs to have the capacity to handle this in it's loss function. An MDN is a good way to handle the predictions of these multiple output values.\n",
    "\n",
    "This implementation owes much to the following:\n",
    "\n",
    "- [David Ha - Mixture Density Networks with TensorFlow](http://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow/)\n",
    "- [Mixture Density Networks in Edward](http://edwardlib.org/tutorials/mixture-density-network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from context import * # imports the MDN layer \n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data\n",
    "\n",
    "Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating some data:\n",
    "NSAMPLE = 5000\n",
    "\n",
    "z_data = np.float32(np.random.uniform(-10.5, 10.5, NSAMPLE))\n",
    "r_data = np.random.normal(size=NSAMPLE)\n",
    "s_data = np.random.normal(size=NSAMPLE)\n",
    "x_data = np.sin(0.75 * z_data) * 7.0 + z_data * 0.5 + r_data * 1.0\n",
    "y_data = np.cos(0.80 * z_data) * 6.5 + z_data * 0.5 + s_data * 1.0\n",
    "\n",
    "x_input = z_data.reshape((NSAMPLE, 1))\n",
    "y_input = np.array([x_data,y_data])\n",
    "y_input = y_input.T #reshape to (NSAMPLE,2)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x_data, y_data, z_data, alpha=0.3, c='r') #c=perf_down_sampled.moving\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the MDN Model\n",
    "\n",
    "Now we will construct the MDN model in Keras. This uses the `Sequential` model interface in Keras.\n",
    "\n",
    "The `MDN` layer comes after one or more `Dense` layers. You need to define the output dimension and number of mixtures for the MDN like so: `MDN(output_dimension, number_mixtures)`.\n",
    "\n",
    "For this problem, we only need an output dimension of 1 as we are predicting one value (y). Adding more mixtures adds a more parameters (model is more complex, takes longer to train), but might help make the solutions better. You can see from the training data that there are at maximum 5 different layers to predict in the curve, so setting `N_MIXES = 5` is a good place to start.\n",
    "\n",
    "For MDNs, we have to use a special loss function that can handle the mixture parameters: the function has to take into account the number of output dimensions and mixtures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_HIDDEN = 15\n",
    "N_MIXES = 10\n",
    "OUTPUT_DIMS = 2\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(N_HIDDEN, batch_input_shape=(None, 1), activation='relu'))\n",
    "model.add(keras.layers.Dense(N_HIDDEN, activation='relu'))\n",
    "model.add(mdn.MDN(OUTPUT_DIMS, N_MIXES))\n",
    "model.compile(loss=mdn.get_mixture_loss_func(OUTPUT_DIMS,N_MIXES), optimizer=keras.optimizers.Adam())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Now we train the model using Keras' normal `fit` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=x_input, y=y_input, batch_size=128, epochs=300, validation_split=0.15, callbacks=[keras.callbacks.TerminateOnNaN()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Loss\n",
    "\n",
    "It's interesting to see how the model trained. We can see that after a certain point improvement in training is rather slow.\n",
    "\n",
    "For this problem a loss value around 3.0 produces quite good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylim([0,9])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out the MDN Model\n",
    "\n",
    "Now we try out the model by making predictions at 3000 evenly spaced points on the x-axis. \n",
    "\n",
    "Mixture models output lists of parameters, so we're going to sample from these parameters for each point on the x-axis, and also try plotting the parameters themselves so we can have some insight into what the model is learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample on some test data:\n",
    "x_test = np.float32(np.arange(-15,15,0.1))\n",
    "NTEST = x_test.size\n",
    "print(\"Testing:\", NTEST, \"samples.\")\n",
    "x_test = x_test.reshape(NTEST,1) # needs to be a matrix, not a vector\n",
    "\n",
    "# Make predictions from the model\n",
    "y_test = model.predict(x_test)\n",
    "# y_test contains parameters for distributions, not actual points on the graph.\n",
    "# To find points on the graph, we need to sample from each distribution.\n",
    "\n",
    "# Split up the mixture parameters (for future fun)\n",
    "mus = np.apply_along_axis((lambda a: a[:N_MIXES*OUTPUT_DIMS]), 1, y_test)\n",
    "sigs = np.apply_along_axis((lambda a: a[N_MIXES*OUTPUT_DIMS:2*N_MIXES*OUTPUT_DIMS]), 1, y_test)\n",
    "pis = np.apply_along_axis((lambda a: mdn.softmax(a[-N_MIXES:])), 1, y_test)\n",
    "\n",
    "# Sample from the predicted distributions\n",
    "y_samples = np.apply_along_axis(mdn.sample_from_output, 1, y_test, OUTPUT_DIMS, N_MIXES, temp=1.0, sigma_temp=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted samples.\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x_data, y_data, z_data, alpha=0.1, c='r') #c=perf_down_sampled.moving\n",
    "ax.scatter(y_samples.T[0], y_samples.T[1], x_test, alpha=0.1, c='b') #c=perf_down_sampled.moving\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the means - this gives us some insight into how the model learns to produce the mixtures.\n",
    "# Cool!\n",
    "\n",
    "# Plot the predicted samples.\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x_data, y_data, z_data, alpha=0.1, c='r') #c=perf_down_sampled.moving\n",
    "ax.scatter(y_samples.T[0], y_samples.T[1], x_test, alpha=0.1, c='b') #c=perf_down_sampled.moving\n",
    "for m in range(N_MIXES):\n",
    "    one_pair = mus[m*OUTPUT_DIMS:(m+1)*OUTPUT_DIMS]\n",
    "    ax.scatter(mus[:,2*m], mus[:,2*m + 1] , x_test, marker='o',alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the variances and weightings of the means as well.\n",
    "\n",
    "# Plot the predicted samples.\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x_data, y_data, z_data, alpha=0.1, c='r') #c=perf_down_sampled.moving\n",
    "for m in range(N_MIXES):\n",
    "    one_pair = mus[m*OUTPUT_DIMS:(m+1)*OUTPUT_DIMS]\n",
    "    ax.scatter(mus[:,2*m], mus[:,2*m + 1] , x_test, s=100*sigs[:,2*m]*pis[:,m], marker='o',alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
